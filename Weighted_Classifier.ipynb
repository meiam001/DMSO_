{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from tensorflow import keras\n",
    "from math import floor\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tf_data_inputs import undersampled_data, entire_df_input_fprint, random_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "\n",
    "PATH = '.'\n",
    "descriptors = 'Descriptors'\n",
    "fing_path = os.path.join(PATH, descriptors)\n",
    "\n",
    "morg_2048_path = 'morgan_2048_df.p'\n",
    "morg_1024_path = 'morgan_1024_df.p'\n",
    "maccs = 'maccs_df.p'\n",
    "\n",
    "morg_2048_bit = os.path.join(fing_path, morg_2048_path)\n",
    "morg_1024_bit = os.path.join(fing_path, morg_1024_path)\n",
    "maccs = os.path.join(fing_path, maccs)\n",
    "\n",
    "train_frac = .8\n",
    "validation_frac = .2\n",
    "test_frac = .2\n",
    "\n",
    "pd.options.display.max_rows = 14\n",
    "pd.options.display.max_columns = 6\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "current_descriptor = 'maccs'\n",
    "second_desc = None\n",
    "sampling_method = 'random'\n",
    "\n",
    "# Current options are fingerprint or combined\n",
    "descriptor = 'fingerprints'\n",
    "descriptor2 = 'combined'\n",
    "\n",
    "# load descriptor\n",
    "df = pickle.load(open(descriptors +'\\\\%s_df.p'%current_descriptor, 'rb'))\n",
    "\n",
    "if second_desc:\n",
    "    df2 = pickle.load(open(descriptors +'\\\\%s_df.p'%second_desc, 'rb'))\n",
    "    df[descriptor] = df[descriptor] + df2[descriptor2]\n",
    "\n",
    "# Randomize dataframe\n",
    "df = df.sample(frac = 1)\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df_len = len(df)\n",
    "\n",
    "if sampling_method == 'undersample':\n",
    "    df[descriptor] = df[descriptor].apply(tuple)\n",
    "    df['Solubility'] = df['Solubility'].apply(tuple)\n",
    "    # Creating training and first validation set\n",
    "    df_training = df[:floor(df_len*train_frac)]\n",
    "    df_training_insol = df_training[df_training['Solubility']==(1,0)]\n",
    "    df_training_sol = df_training[df_training['Solubility']==(0,1)]\n",
    "    validation_set1 = df[floor(df_len*train_frac):floor(df_len*train_frac)+floor(df_len*validation_frac)]\n",
    "    del df\n",
    "    \n",
    "validation_set1 = df[floor(df_len*train_frac):floor(df_len*train_frac)+floor(df_len*validation_frac)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len([c for c, i in enumerate(df_training['Solubility']) if i == (1,0)])\n",
    "# /len([c for c, i in enumerate(df_training['Solubility']) if i == (0,1)])*100,'% insoluble')\n",
    "# print(len([c for c, i in enumerate(df_training['Solubility']) if i == (1,0)]), '# insoluble')\n",
    "# print(len([c for c, i in enumerate(df_training['Solubility']) if i == (0,1)]), '# soluble')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "            / labels.shape[0])\n",
    "\n",
    "def insol_accuracy(valid, labels_v):\n",
    "    valid = [list(i) for i in valid]\n",
    "    labels_v = [list(i) for i in labels_v]\n",
    "    neg_matching = [count for count in range(len(valid)) if valid[count] == [1,0] and labels_v[count] == [1,0]]\n",
    "    return len(neg_matching)/len([count for count in range(len(labels_v)) if labels_v[count] == [1,0]])*100\n",
    "\n",
    "def perf_measure(y_hat, y_actual):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "    for i in range(len(y_hat)): \n",
    "        if list(y_hat[i])==list(y_actual[i])==[1,0]:\n",
    "            TN +=1\n",
    "        elif list(y_hat[i])==list(y_actual[i])==[0,1]:\n",
    "            TP +=1\n",
    "        elif list(y_hat[i])!=list(y_actual[i])==[1,0]:\n",
    "            FN +=1\n",
    "        elif list(y_hat[i])!=list(y_actual[i])==[0,1]:\n",
    "            FP +=1\n",
    "    return TP, FP, TN, FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "if sampling_method == 'undersample':\n",
    "    n_inputs = len(df_training[descriptor][0])\n",
    "else:\n",
    "    n_inputs = len(df[descriptor][0])\n",
    "    \n",
    "layer1_nodes = 4100\n",
    "layer2_nodes = 3000\n",
    "layer3_nodes = 2300\n",
    "\n",
    "batch_size = 300\n",
    "learning_rate = .01\n",
    "l1 = 0\n",
    "l2 = .01\n",
    "training_weights = [2,.09]\n",
    "\n",
    "\n",
    "with graph.as_default():\n",
    "    # Setting up tensorflow graph\n",
    "    # Training data to be fed at runtime\n",
    "    train_data = tf.placeholder(dtype=tf.float32, name='input_layer')\n",
    "    train_labels = tf.placeholder(dtype=tf.float32, name='train_labels')\n",
    "\n",
    "    \n",
    "    # Weights\n",
    "    layer1_weights = tf.Variable(tf.truncated_normal([n_inputs, layer1_nodes]), name='l1_weights')\n",
    "    layer2_weights = tf.Variable(tf.truncated_normal([layer1_nodes, layer2_nodes]), name='l2_weights')\n",
    "    layer4_weights = tf.Variable(tf.truncated_normal([layer2_nodes, 2]), name='l2_weights')\n",
    "    \n",
    "    # Logits\n",
    "    logit1 = tf.matmul(train_data, layer1_weights, name='logit1')\n",
    "    relu_layer1 = tf.nn.relu(logit1, name='relu_layer')\n",
    "    # dropout_1 = tf.nn.dropout(x=relu_layer1, keep_prob=.5, name='dropout')\n",
    "    \n",
    "    logit2 = tf.matmul(relu_layer1, layer2_weights, name='logit2')\n",
    "    relu_layer2 = tf.nn.relu(logit2, name='relu_layer')\n",
    "    # dropout_2 = tf.nn.dropout(x=relu_layer2, keep_prob=.5, name='dropout')\n",
    "    \n",
    "    logit4 = tf.matmul(relu_layer2, layer4_weights, name='logit4')\n",
    "    scaled_logits = tf.multiply(logit4, training_weights)\n",
    "    \n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=scaled_logits, labels=train_labels)\n",
    "    \n",
    "    total_loss = tf.reduce_mean(loss, name='loss')\n",
    "    \n",
    "    optimizer = tf.train.FtrlOptimizer(\n",
    "        learning_rate, \n",
    "        l2_regularization_strength=l2, \n",
    "        l1_regularization_strength=l1,\n",
    "        name='optimizer'\n",
    "    ).minimize(total_loss)\n",
    "    \n",
    "    # Prediction\n",
    "    _dummy = tf.constant(5)\n",
    "    train_prediction = tf.nn.softmax(logit4, name='predictor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    # writer = tf.summary.FileWriter('.', session.graph)\n",
    "    for step in range(10001):\n",
    "        data, labels = random_sample(df, descriptor)\n",
    "        feed_dict = {train_data: data, train_labels: labels }\n",
    "        _,_2  = session.run([optimizer, _dummy], feed_dict=feed_dict)\n",
    "        #print(predictions)\n",
    "        #print(l)\n",
    "        if step % 100 == 0:\n",
    "            data, labels = entire_df_input_fprint(validation_set1, descriptor)\n",
    "            feed_dict = {train_data: data, train_labels: labels }\n",
    "            _, predictions, l = session.run([_dummy, train_prediction, total_loss], feed_dict=feed_dict)\n",
    "    #              valid_labels, valid_features = entire_df_input_fprint(validation_set1, descriptor)\n",
    "            print(perf_measure(predictions, labels))\n",
    "            print('accuracy at step {}: {:.2f}'.format(step, accuracy(predictions, labels)))\n",
    "            print('loss at step {}: {:.2f}'.format(step,l))\n",
    "            print('')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    print(session.run(tf.multiply([[1,2],[4,5]],[2,2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rdkit)",
   "language": "python",
   "name": "rdkit_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
