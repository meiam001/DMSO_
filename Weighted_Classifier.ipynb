{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from tensorflow import keras\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "\n",
    "PATH = '.'\n",
    "FINGERPRINTS = 'fingerprints'\n",
    "fing_path = os.path.join(PATH, FINGERPRINTS)\n",
    "\n",
    "morg_2048_path = 'morgan_2048_df.p'\n",
    "morg_1024_path = 'morgan_1024_df.p'\n",
    "maccs = 'maccs_df.p'\n",
    "\n",
    "morg_2048_bit = os.path.join(fing_path, morg_2048_path)\n",
    "morg_1024_bit = os.path.join(fing_path, morg_1024_path)\n",
    "maccs = os.path.join(fing_path, maccs)\n",
    "\n",
    "train_frac = .8\n",
    "validation_frac = .2\n",
    "test_frac = .2\n",
    "\n",
    "pd.options.display.max_rows = 14\n",
    "pd.options.display.max_columns = 6\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combined_fp():\n",
    "    current_fingerprint = 'pubchem'\n",
    "    # Combining fingerprints\n",
    "    df = pickle.load(open(FINGERPRINTS+'\\\\%s_df.p'%current_fingerprint, 'rb'))\n",
    "\n",
    "    current_fingerprint = 'maccs'\n",
    "    # Combining fingerprints\n",
    "    df2 = pickle.load(open(FINGERPRINTS+'\\\\%s_df.p'%current_fingerprint, 'rb'))\n",
    "    df['fingerprints'] = df['fingerprints'] + df2['fingerprints']\n",
    "    del df2\n",
    "\n",
    "    current_fingerprint = 'morgan_2048'\n",
    "    # Combining fingerprints\n",
    "    df3 = pickle.load(open(FINGERPRINTS+'\\\\%s_df.p'%current_fingerprint, 'rb'))\n",
    "    df['fingerprints'] = df['fingerprints'] + df3['fingerprints']\n",
    "    del df3\n",
    "    \n",
    "    pickle.dump(df, open('Fingerprints\\\\combined_df.p', 'wb+'))\n",
    "# combined_fp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "current_fingerprint = 'maccs'\n",
    "\n",
    "# load fingerprints\n",
    "df = pickle.load(open(FINGERPRINTS+'\\\\%s_df.p'%current_fingerprint, 'rb'))\n",
    "# Randomize dataframe\n",
    "df = df.sample(frac = 1)\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_len = len(df)\n",
    "df['Solubility'] = df['Solubility'].apply(tuple)\n",
    "df['fingerprints'] = df['fingerprints'].apply(tuple)\n",
    "\n",
    "# Creating training and first validation set\n",
    "df_training = df[:floor(df_len*train_frac)]\n",
    "df_training_insol = df_training[df_training['Solubility']==(1,0)]\n",
    "df_training_sol = df_training[df_training['Solubility']==(0,1)]\n",
    "\n",
    "validation_set1 = df[floor(df_len*train_frac):floor(df_len*train_frac)+floor(df_len*validation_frac)]\n",
    "\n",
    "# Delete original set to reduce overhead.\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.631635234890576 % insoluble\n",
      "2159 # insoluble\n",
      "38337 # soluble\n"
     ]
    }
   ],
   "source": [
    "print(len([c for c, i in enumerate(df_training['Solubility']) if i == (1,0)])\n",
    "/len([c for c, i in enumerate(df_training['Solubility']) if i == (0,1)])*100,'% insoluble')\n",
    "print(len([c for c, i in enumerate(df_training['Solubility']) if i == (1,0)]), '# insoluble')\n",
    "print(len([c for c, i in enumerate(df_training['Solubility']) if i == (0,1)]), '# soluble')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch(df_i,df_s):\n",
    "    \"\"\"\n",
    "    Feed function for model. Undersampled, randomly pickes 100 insoluble and 100 soluble compounds\n",
    "    \"\"\"\n",
    "    insol = df_i.sample(n = 150)\n",
    "    sol = df_s.sample(n = 150)\n",
    "    \n",
    "    df = pd.concat([insol, sol])\n",
    "    df = df.sample(frac=1)\n",
    "    \n",
    "    fprints = np.array(list(df['fingerprints']), dtype='int32')\n",
    "    labels = np.array(list(df['Solubility']), dtype='int32')\n",
    "    return fprints, labels\n",
    "\n",
    "def validation_input(valid_set):\n",
    "    features = np.array(list(valid_set['fingerprints']))\n",
    "    labels = np.array(list(valid_set['Solubility']))\n",
    "    return features, labels\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "            / labels.shape[0])\n",
    "\n",
    "def insol_accuracy(valid, labels_v):\n",
    "    valid = [list(i) for i in valid]\n",
    "    labels_v = [list(i) for i in labels_v]\n",
    "    neg_matching = [count for count in range(len(valid)) if valid[count] == [1,0] and labels_v[count] == [1,0]]\n",
    "    return len(neg_matching)/len([count for count in range(len(labels_v)) if labels_v[count] == [1,0]])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perf_measure(y_actual, y_hat):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    for i in range(len(y_hat)): \n",
    "        if y_actual[i]==y_hat[i]==1:\n",
    "           TP += 1\n",
    "        if y_hat[i]==1 and y_actual[i]!=y_hat[i]:\n",
    "           FP += 1\n",
    "        if y_actual[i]==y_hat[i]==0:\n",
    "           TN += 1\n",
    "        if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n",
    "           FN += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network time!\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "n_inputs = len(df_training['fingerprints'][0])\n",
    "layer1_nodes = 4100\n",
    "layer2_nodes = 3000\n",
    "layer3_nodes = 2300\n",
    "\n",
    "batch_size = 300\n",
    "learning_rate = .001\n",
    "l1 = 0\n",
    "l2 = .01\n",
    "\n",
    "with graph.as_default():\n",
    "    # Setting up tensorflow graph\n",
    "    # Training data to be fed at runtime\n",
    "    train_data = tf.placeholder(dtype=tf.float32, name='input_layer')\n",
    "    train_labels = tf.placeholder(dtype=tf.float32, name='train_labels')\n",
    "\n",
    "    \n",
    "    # Weights\n",
    "    layer1_weights = tf.Variable(tf.truncated_normal([n_inputs, layer1_nodes]), name='l1_weights')\n",
    "    layer2_weights = tf.Variable(tf.truncated_normal([layer1_nodes, layer2_nodes]), name='l2_weights')\n",
    "    layer4_weights = tf.Variable(tf.truncated_normal([layer2_nodes, 2]), name='l2_weights')\n",
    "    \n",
    "    # Logits\n",
    "    logit1 = tf.matmul(train_data, layer1_weights, name='logit1')\n",
    "    relu_layer1 = tf.nn.relu(logit1, name='relu_layer')\n",
    "    # dropout_1 = tf.nn.dropout(x=relu_layer1, keep_prob=.5, name='dropout')\n",
    "    \n",
    "    logit2 = tf.matmul(relu_layer1, layer2_weights, name='logit2')\n",
    "    relu_layer2 = tf.nn.relu(logit2, name='relu_layer')\n",
    "    # dropout_2 = tf.nn.dropout(x=relu_layer2, keep_prob=.5, name='dropout')\n",
    "    \n",
    "    logit4 = tf.matmul(relu_layer2, layer4_weights, name='logit4')\n",
    "    \n",
    "    # Loss tf.nn.softmax_cross_entropy_with_logits_v2\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit4, labels=train_labels), name='loss')\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.FtrlOptimizer(\n",
    "        learning_rate, \n",
    "        l2_regularization_strength=l2, \n",
    "        l1_regularization_strength=l1,\n",
    "        name='optimizer'\n",
    "    ).minimize(loss)\n",
    "    \n",
    "    # Prediction\n",
    "    _test = tf.constant(5)\n",
    "    train_prediction = tf.nn.softmax(logit4, name='predictor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "Minibatch loss at step 0: 5652.017578125\n",
      "Minibatch accuracy: 51.00\n",
      "Insoluble minibatch accuracy: 96.67\n",
      "Insoluble validation accuracy: 4.98\n",
      "Minibatch validation accuracy: 85.41\n",
      "\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "Minibatch loss at step 100: 630.0445556640625\n",
      "Minibatch accuracy: 63.33\n",
      "Insoluble minibatch accuracy: 81.33\n",
      "Insoluble validation accuracy: 43.10\n",
      "Minibatch validation accuracy: 75.57\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-62778fa8180a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_training_insol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_training_sol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_prediction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[1;31m# print(predictions)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;31m# print(l)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\rdkit_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    875\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 877\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    878\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\rdkit_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1100\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1101\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\rdkit_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1272\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1273\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1274\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\rdkit_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1276\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1277\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1278\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1279\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\rdkit_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1263\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\rdkit_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    # writer = tf.summary.FileWriter('.', session.graph)\n",
    "    for step in range(10001):\n",
    "        data, labels = batch(df_training_insol, df_training_sol)\n",
    "        feed_dict = {train_data: data,train_labels: labels }\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        # print(predictions)\n",
    "        # print(l)\n",
    "        if step % 100 == 0:\n",
    "            data_v, labels_v = validation_input(validation_set1)\n",
    "            feed_dict = {train_data: data_v, train_labels: labels_v}\n",
    "            _, valid = session.run([_test, train_prediction], feed_dict=feed_dict)\n",
    "            print(\"Minibatch loss at step {}: {}\".format(step, l))\n",
    "            print(\"Minibatch accuracy: {:.2f}\".format(accuracy(predictions, labels)))\n",
    "            print(\"Insoluble minibatch accuracy: {:.2f}\".format(insol_accuracy(predictions, labels)))\n",
    "            print(\"Insoluble validation accuracy: {:.2f}\".format(insol_accuracy(valid, labels_v)))\n",
    "            print(\"Minibatch validation accuracy: {:.2f}\".format(accuracy(valid, labels_v)))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rdkit)",
   "language": "python",
   "name": "rdkit_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
